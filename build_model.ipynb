{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to be able to predict the number of rides in the local bike share program using forecasting methods that take historical data into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing ride data\n",
    "\n",
    "HealthyRide is the Pittsburgh bike share program. Data for all rides is made available at:\n",
    "\n",
    "> https://healthyridepgh.com/data/\n",
    "\n",
    "A file might have a first few lines like:\n",
    "\n",
    "```\n",
    "Trip id,Starttime,Stoptime,Bikeid,Tripduration,From station id,From station name,To station id,To station name,Usertype\n",
    "15335599,1/1/2016 1:44,1/1/2016 2:01,70294,1068,1026,Penn Ave & S Whitfield St,1032,Walnut St & College St,\n",
    "15335629,1/1/2016 2:39,1/1/2016 2:53,70360,892,1029,Alder St & S Highland Ave,1021,Taylor St & Liberty Ave,\n",
    "```\n",
    "\n",
    "The ride data from 2015 - 2017 are available in this project: \n",
    "\n",
    "```\n",
    "datasets/\n",
    "    HealthyRide Rentals 2015 Q2.csv\n",
    "    HealthyRide Rentals 2015 Q3.csv\n",
    "    HealthyRide Rentals 2015 Q4.csv\n",
    "    HealthyRide Rentals 2016 Q1.csv\n",
    "    HealthyRide Rentals 2016 Q2.csv\n",
    "    HealthyRide Rentals 2016 Q3.csv\n",
    "    HealthyRide Rentals 2016 Q4.csv\n",
    "    HealthyRide Rentals 2017 Q1.csv\n",
    "    HealthyRide Rentals 2017 Q2.csv\n",
    "    HealthyRide Rentals 2017 Q3.csv\n",
    "    HealthyRide Rentals 2017 Q4.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "rides = dd.read_csv('data/HealthyRide Rentals 201*.csv')\n",
    "\n",
    "for c in 'Starttime','Stoptime':\n",
    "    rides[c] = dd.to_datetime(rides[c], format='%m/%d/%Y %H:%M')\n",
    "    \n",
    "\n",
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning ride data and aggregate\n",
    "\n",
    "There are times when cities have bike share services that a rider gives up on a ride, for whatever reason.  This would generally be reflected by them checking out a bike, but quickly realizing they cannot complete their ride, immediately returning it in the same place.  The data provides trip time in units of seconds.\n",
    "\n",
    "Merely checking out and returning a bike to the same location does not mean the same thing.  For example, if a station is close to a resident's house, they might check a bike out, ride for an hour to complete errands, or just for leisure, and then come back to the same place.  We do not think of this as a \"failed ride.\"\n",
    "\n",
    "This cleaning step removes the data for \"failed rides\" from the DataFrame named `rides`.  A failed ride is defined as one:\n",
    "\n",
    "1. From and to the same station\n",
    "2. Duration of less than 120 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short = rides['Tripduration'] < 120\n",
    "round_trip = rides['From station id'] == rides['To station id']\n",
    "failed = short & round_trip\n",
    "cleaned = rides[~failed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample is a sophisticated groupby-like aggregation method in Pandas (and Dask). It will aggregate data according to a *frequency*. In this case the raw transactions are resampled to provide a count of the number of rides taken each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = cleaned.set_index('Starttime')['Trip id'].resample('D').count()\n",
    "daily.name = 'Count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can clearly see the trends in daily ridership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.dask\n",
    "\n",
    "daily.hvplot.line(width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series analysis: seasonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Statsmodels](https://www.statsmodels.org/stable/index.html) provides several useful methodology for timeseries analysis and modeling. We'll begin by attempting to decompose the average daily ride counts per week.\n",
    "\n",
    "As should be obvious there is a strong seasonal variation yearly. Notice the small spread in the Trend values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "weekly = daily.resample('W').mean().compute().asfreq('W-SUN')\n",
    "\n",
    "result = seasonal_decompose(weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "\n",
    "res = pd.DataFrame({\n",
    "        'observed': result.observed,\n",
    "        'seasonal': result.seasonal,\n",
    "        'trend': result.trend,\n",
    "        'residual': result.resid,\n",
    "})\n",
    "\n",
    "res.hvplot.line(subplots=True, height=200).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit a forecasting model the data must first be *stationary*, meaning that statistical properties like mean, variance, or autocorrelation are constant over time.\n",
    "\n",
    "A standard test for stationarity is the [Augmented Dickey-Fuller](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test). The p-value is quite small so the weekly average series looks stationary so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "results = adfuller(weekly)\n",
    "results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the auto-correlation and partial auto-correlation functions will help us build the forecasting model later. \n",
    "\n",
    "There is a strong auto correlation every week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plot_acf(weekly, lags=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we plot the change in rides each week the correlation drops much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = weekly.diff().dropna()\n",
    "plot_acf(diff, lags=60);\n",
    "plot_pacf(diff, lags=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to build a model that takes into account some amount of previous data. This is referred to as lags.\n",
    "\n",
    "Three important parameters for the ARIMA model are\n",
    "\n",
    "* **p** is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model. Intuitively, this would be similar to stating that it is likely to be warm tomorrow if it has been warm the past 3 days.\n",
    "* **d** is the integrated part of the model. This includes terms in the model that incorporate the amount of differencing (i.e. the number of past time points to subtract from the current value) to apply to the time series. Intuitively, this would be similar to stating that it is likely to be same temperature tomorrow if the difference in temperature in the last three days has been very small.\n",
    "* **q** is the moving average part of the model. This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past.\n",
    "\n",
    "For convenience we'll use the [SARIMAX](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html) method, but not include a seasonal component.\n",
    "\n",
    "From the plots above we know that `d` should be 1, and both plots drop to zero quickly, so `p` and `q` can be set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "p = 2\n",
    "d = 1\n",
    "q = 2\n",
    "order = (p, d, q)\n",
    "\n",
    "arima = SARIMAX(weekly, order=order)\n",
    "model = arima.fit(disp=0)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the original data and the fitted model shows close agreement with the forecasting model.\n",
    "\n",
    "**Note**: the predictions here are made using the *real* historical data at each step. This is referred to as *one-step-ahead* prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict()\n",
    "\n",
    "weekly.hvplot.line(label='actual') * predicted.hvplot.line(label='one-step-ahead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit above looks good, but the *whole* data was used to train the model. Let's build a new model using only the first two years of data and use that to perform one-step-ahead predictions.\n",
    "\n",
    "The `trained_model` has the parameters we wish to validate using unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = weekly.loc['2015':'2016']\n",
    "\n",
    "arima = SARIMAX(train, order=order)\n",
    "trained_model = arima.fit(disp=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize the `.predict()` method we'll apply the pre-trained parameters to a model with *all* of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_arima = SARIMAX(weekly, order=order)\n",
    "testing_model = _arima.filter(trained_model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the one-step-ahead predictions are made using the actual previous values, but the model was not fit with that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = testing_model.get_prediction(full_results=True).summary_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual    = weekly.hvplot.line(label='actual')\n",
    "one_ahead = predicted.loc['2017':, 'mean'].hvplot.line(label='one-step-ahead')\n",
    "ci95      = predicted.loc['2017':].hvplot.area(y='mean_ci_lower', y2='mean_ci_upper',\n",
    "                                               alpha=0.2, label='95% confidence')\n",
    "\n",
    "(actual * one_ahead * ci95).opts(width=800, legend_position='top_left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model.save('bike_forecasts.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook the model is loaded and forecasting function is defined. Click the blue link to open the notebook.\n",
    "\n",
    "<a href='load_model.ipynb' class='btn btn-primary btn-lg'>Load model and predict</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<font color='grey'><i>Copyright Anaconda 2012-2019 All Rights Reserved.</i></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
